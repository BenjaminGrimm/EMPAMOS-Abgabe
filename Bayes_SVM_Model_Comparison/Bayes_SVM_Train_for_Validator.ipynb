{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font size=\"6\" weight=\"bold\">Bayes Trainer for Validator</font>__\n",
    "<br>\n",
    "<strong>Not for production</strong>\n",
    "Hier werden die Modelle für die SVM und den Bayes angelernt. <br>\n",
    "Dazu werden alle Pattern durchlaufen. Für jedes Pattern wird ein DataFrame mit positiven und negativen Sätzen erstellt. <br>\n",
    "In diesem Notebook werden nur die Ergebnisse des besten Bayes(siehe Dokumenttion) und der besten SVM(siehe Dokumentation) neu trainiert,beide werden mit den gleichen Sätzen trainiert, diese können sich jedoch zwischen Bays und SVM unterscheiden. Zum Beispiel sind beim besten Bayes die Stoppwörter entfernt worden, jedoch wurden die Sätze nicht lemmatisiert. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisieren der benötigten Klassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pickle\n",
    "import os \n",
    "import os.path as path\n",
    "from joblib import dump, load\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "\n",
    "%run REST.ipynb\n",
    "client = empamos_rest_client()\n",
    "%run TrainingSetFactory_for_BayesTrainer_lemma_no_lemma_stopwords_no_stopwords.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Tokenisieren mit Stoppwörter/Tokenisieren ohne Stoppwörter/Nicht Tokenisieren mit Stoppwörter/Nicht Tokenisieren ohne Stoppwörter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Ordner zur Ablage der Modelle und des Outputs\n",
    "directoryname = \"SVM_Bayes_comparison\"\n",
    "os.makedirs(directoryname, exist_ok=True)\n",
    "directoryname_output = \"SVM_Bayes_comparison/Output\"\n",
    "os.makedirs(directoryname_output, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1292\n",
      "1409\n",
      "Keine Daten vorhanden! 1409\n",
      "1417\n",
      "Keine Daten vorhanden! 1417\n",
      "1433\n",
      "Keine Daten vorhanden! 1433\n",
      "1446\n",
      "Keine Daten vorhanden! 1446\n",
      "1455\n",
      "Keine Daten vorhanden! 1455\n",
      "1464\n",
      "Keine Daten vorhanden! 1464\n",
      "1473\n",
      "Keine Daten vorhanden! 1473\n",
      "1482\n",
      "Keine Daten vorhanden! 1482\n",
      "1492\n",
      "1515\n",
      "Keine Daten vorhanden! 1515\n",
      "1665\n",
      "Keine Daten vorhanden! 1665\n",
      "1756\n",
      "Keine Daten vorhanden! 1756\n",
      "2884\n",
      "Keine Daten vorhanden! 2884\n",
      "2894\n",
      "Keine Daten vorhanden! 2894\n",
      "2907\n",
      "Keine Daten vorhanden! 2907\n",
      "2968\n",
      "Keine Daten vorhanden! 2968\n",
      "2981\n",
      "Keine Daten vorhanden! 2981\n",
      "3085\n",
      "Keine Daten vorhanden! 3085\n",
      "3098\n",
      "Keine Daten vorhanden! 3098\n",
      "3142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3173\n",
      "3194\n",
      "Keine Daten vorhanden! 3194\n",
      "3285\n",
      "Keine Daten vorhanden! 3285\n",
      "3312\n",
      "Keine Daten vorhanden! 3312\n",
      "3377\n",
      "Keine Daten vorhanden! 3377\n",
      "3755\n",
      "Keine Daten vorhanden! 3755\n",
      "4426\n",
      "Keine Daten vorhanden! 4426\n",
      "5718\n",
      "Keine Daten vorhanden! 5718\n",
      "5729\n",
      "Keine Daten vorhanden! 5729\n",
      "5740\n",
      "Keine Daten vorhanden! 5740\n",
      "7090\n",
      "7098\n",
      "Keine Daten vorhanden! 7098\n",
      "7106\n",
      "Keine Daten vorhanden! 7106\n",
      "7115\n",
      "Keine Daten vorhanden! 7115\n",
      "7123\n",
      "Keine Daten vorhanden! 7123\n",
      "7227\n",
      "Keine Daten vorhanden! 7227\n",
      "8520\n",
      "Keine Daten vorhanden! 8520\n",
      "8528\n",
      "8536\n",
      "8544\n",
      "Keine Daten vorhanden! 8544\n",
      "8552\n",
      "Keine Daten vorhanden! 8552\n",
      "8560\n",
      "Keine Daten vorhanden! 8560\n",
      "8568\n",
      "Keine Daten vorhanden! 8568\n",
      "8576\n",
      "Keine Daten vorhanden! 8576\n",
      "8584\n",
      "Keine Daten vorhanden! 8584\n",
      "8592\n",
      "Keine Daten vorhanden! 8592\n",
      "8608\n",
      "Keine Daten vorhanden! 8608\n",
      "8616\n",
      "Keine Daten vorhanden! 8616\n",
      "8624\n",
      "Keine Daten vorhanden! 8624\n",
      "8632\n",
      "Keine Daten vorhanden! 8632\n",
      "8640\n",
      "Keine Daten vorhanden! 8640\n",
      "8656\n",
      "Keine Daten vorhanden! 8656\n",
      "8664\n",
      "Keine Daten vorhanden! 8664\n",
      "8680\n",
      "Keine Daten vorhanden! 8680\n",
      "8688\n",
      "Keine Daten vorhanden! 8688\n",
      "8712\n",
      "Keine Daten vorhanden! 8712\n",
      "8720\n",
      "Keine Daten vorhanden! 8720\n",
      "8740\n",
      "Keine Daten vorhanden! 8740\n",
      "8749\n",
      "Keine Daten vorhanden! 8749\n",
      "8767\n",
      "Keine Daten vorhanden! 8767\n",
      "23101\n",
      "Keine Daten vorhanden! 23101\n",
      "23187\n",
      "Keine Daten vorhanden! 23187\n",
      "23211\n",
      "Keine Daten vorhanden! 23211\n",
      "26931\n",
      "Keine Daten vorhanden! 26931\n",
      "26949\n",
      "Keine Daten vorhanden! 26949\n",
      "33875\n",
      "Keine Daten vorhanden! 33875\n",
      "33933\n",
      "Keine Daten vorhanden! 33933\n",
      "34403\n",
      "Keine Daten vorhanden! 34403\n",
      "34480\n",
      "Keine Daten vorhanden! 34480\n",
      "34495\n",
      "Keine Daten vorhanden! 34495\n",
      "35141\n",
      "Keine Daten vorhanden! 35141\n",
      "35703\n",
      "Keine Daten vorhanden! 35703\n",
      "38364\n",
      "Keine Daten vorhanden! 38364\n",
      "39858\n",
      "Keine Daten vorhanden! 39858\n",
      "66662\n",
      "Keine Daten vorhanden! 66662\n",
      "67129\n",
      "Keine Daten vorhanden! 67129\n",
      "73230\n",
      "Keine Daten vorhanden! 73230\n",
      "73244\n",
      "Keine Daten vorhanden! 73244\n",
      "86746\n",
      "Keine Daten vorhanden! 86746\n",
      "86774\n",
      "Keine Daten vorhanden! 86774\n",
      "86801\n",
      "Keine Daten vorhanden! 86801\n",
      "86813\n",
      "Keine Daten vorhanden! 86813\n",
      "101011\n",
      "Keine Daten vorhanden! 101011\n",
      "112052\n",
      "Keine Daten vorhanden! 112052\n",
      "112066\n",
      "Keine Daten vorhanden! 112066\n",
      "164466\n",
      "165619\n",
      "Keine Daten vorhanden! 165619\n",
      "166903\n",
      "167602\n",
      "Keine Daten vorhanden! 167602\n",
      "168482\n",
      "Keine Daten vorhanden! 168482\n",
      "169190\n",
      "Keine Daten vorhanden! 169190\n",
      "171714\n",
      "Keine Daten vorhanden! 171714\n",
      "174222\n",
      "Keine Daten vorhanden! 174222\n",
      "175215\n",
      "Keine Daten vorhanden! 175215\n",
      "175223\n",
      "Keine Daten vorhanden! 175223\n",
      "175231\n",
      "Keine Daten vorhanden! 175231\n",
      "175239\n",
      "Keine Daten vorhanden! 175239\n",
      "175613\n",
      "Keine Daten vorhanden! 175613\n",
      "176363\n",
      "Keine Daten vorhanden! 176363\n",
      "181163\n",
      "Keine Daten vorhanden! 181163\n",
      "182950\n",
      "Keine Daten vorhanden! 182950\n",
      "183168\n",
      "Keine Daten vorhanden! 183168\n",
      "183664\n",
      "Keine Daten vorhanden! 183664\n",
      "210095\n",
      "Keine Daten vorhanden! 210095\n",
      "211189\n",
      "Keine Daten vorhanden! 211189\n"
     ]
    }
   ],
   "source": [
    "factory = TrainingSetFactory(4)\n",
    "pattern_ids = [pattern['pattern_id'] for pattern in client.get_patterns()]\n",
    "\n",
    "for id in pattern_ids:\n",
    "    print(id)\n",
    "    df = factory.trainings_dataframe(id) \n",
    "    if (factory.isValid):\n",
    "        #Dataframe for bayes\n",
    "        dfPos = factory.Sentences_to_DF(factory.no_tokenized_removed_stopwords_pos_sentences,1)\n",
    "        dfNeg = factory.Sentences_to_DF(factory.no_tokenized_removed_stopwords_neg_sentences,0)\n",
    "        df = dfPos.append(dfNeg)\n",
    "        X=df[0]\n",
    "        Y=df[1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.85, random_state=0)\n",
    "        Bayes_Train(id,X_train, X_test, y_train, y_test,\"without_Stopwords_no_lemmatize\")\n",
    "        #Dataframe for SVM\n",
    "        dfPos = factory.Sentences_to_DF(factory.tokenized_stopwords_removed_pos_sentences,1)\n",
    "        dfNeg = factory.Sentences_to_DF(factory.tokenized_stopwords_removed_neg_sentences,0)\n",
    "        df = dfPos.append(dfNeg)\n",
    "        X=df[0]\n",
    "        Y=df[1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.85, random_state=0)\n",
    "        SVM_Train(id,X_train, X_test, y_train, y_test,\"stopwords_removed_lemmatize\")\n",
    "    else:\n",
    "        print(f'Keine Daten vorhanden! {id}')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVM_Train(id,X_train, X_test, y_train, y_test,msg=\"\"):\n",
    "    lsvc = Pipeline([('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', LinearSVC(random_state=None)),\n",
    "       ])\n",
    "\n",
    "    parameters = {\n",
    "         \"vect__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "         \"tfidf__use_idf\": (True, False),\n",
    "         \"clf__loss\" : ['hinge', 'squared_hinge'],\n",
    "         \"clf__tol\" : [1e-6, 1e-5, 1e-4]\n",
    "    }\n",
    "\n",
    "    ##Hier zu betrachtendes Attribut über scoring = <attribute> wählen\n",
    "    gs_clf = GridSearchCV(lsvc, parameters, cv=5, n_jobs=-1, scoring = 'accuracy')\n",
    "    gs_clf = gs_clf.fit(X_train, y_train)\n",
    "    #predit test\n",
    "    predictions = gs_clf.predict(X_test)\n",
    "    pickle.dump(gs_clf.best_estimator_,open(directoryname+\"/SVM_Pattern_Model_\"+msg+str(id),\"wb\"))\n",
    "    #\n",
    "    with open(directoryname_output+\"/SVM_Pattern_Model_evaluation_\"+msg+str(id),\"w\") as file:\n",
    "            file.write(\"Accuracy score\"+ str (accuracy_score(y_test, predictions))+\"\\n\")\n",
    "            file.write('Precision score: ' + str (precision_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "            file.write('Recall score: ' + str (recall_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "            file.write('f1_score score: ' + str (f1_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bayes_Train(id,X_train, X_test, y_train, y_test,msg=\"\"):\n",
    "        naive_bayes = BernoulliNB()\n",
    "        #wordvetor erstellung\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        x_train = vectorizer.fit_transform(X_train)\n",
    "        x_test = vectorizer.transform(X_test)\n",
    "        pickle.dump(vectorizer,open(directoryname+\"/Bayes_sparse_matrix_\"+msg+str(id),\"wb\"))\n",
    "        #trainieren\n",
    "        naive_bayes.fit(x_train, y_train)\n",
    "        predictions = naive_bayes.predict(x_test)\n",
    "        with open(directoryname_output+\"/Bayes_Pattern_Model_evaluation_\"+msg+str(id),\"w\") as file:\n",
    "            file.write(\"Accuracy score\"+ str (accuracy_score(y_test, predictions))+\"\\n\")\n",
    "            file.write('Precision score: ' + str (precision_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "            file.write('Recall score: ' + str (recall_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "            file.write('f1_score score: ' + str (f1_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "        pickle.dump(naive_bayes,open(directoryname+\"/Bayes_Pattern_Model_\"+msg+str(id),\"wb\"))\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
