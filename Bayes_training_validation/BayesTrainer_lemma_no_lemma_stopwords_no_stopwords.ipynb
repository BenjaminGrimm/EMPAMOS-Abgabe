{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font size=\"6\" weight=\"bold\">Bayes Trainer for Validator</font>__\n",
    "<br>\n",
    "<strong>Not for production!</strong>\n",
    "Hier werden die Modelle für den Bayes angelernt. in einem Durchgang werden insgesamt 4 verschiedene Modelle trainiert: \n",
    "<br><br>Nicht lemmatisiert und keine Stoppwörter entfernt\n",
    "<br> Nicht lemmatisiert und Stoppwörter entfernt\n",
    "<br> lemmatisiert und keine Stoppwörter entfernt\n",
    "<br> lemmatisiert und  Stoppwörter entfernt<br>\n",
    "<br>\n",
    "Dazu werden alle Pattern durchlaufen. Für jedes Pattern wird ein DataFrame mit positiven und negativen Sätzen erstellt. Dieser Dataframe wird dann in X Test und Y Test aufgeteilt. Hierbei werden immer die ersten 85% fürs training genutzt und die restlichen 15% für das Testen. Damit kann sicher gestellt werden, dass alle Modelle identisch sind von den Daten.\n",
    "<br>\n",
    "Für jedes Modell wird ein eigener Ordner angelegt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisieren der benötigten Klassen und Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import os \n",
    "import os.path as path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run REST.ipynb\n",
    "%run TrainingSetFactory_for_BayesTrainer_lemma_no_lemma_stopwords_no_stopwords.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Trainer not for production only for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesTrainer: \n",
    "    directoryname=\"Bayes\"\n",
    "    directoryname_output =\"Bayes_Output\"\n",
    "\n",
    "    def train(self, id, dataframe):        \n",
    "        #create directory if not exist\n",
    "        os.makedirs(self.directoryname, exist_ok=True)\n",
    "        os.makedirs(self.directoryname+\"/\" +self.directoryname_output, exist_ok=True)\n",
    "        bayes_file = self.get_filename(id,self.directoryname,\"Bayes\")\n",
    "        countervector_file = self.get_filename(id,self.directoryname,\"Counter\")\n",
    "        X = dataframe[0]\n",
    "        Y = dataframe[1]\n",
    "        # random_state = 0 to ensure that every instance has the same training and test data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.85, random_state=0)\n",
    "        naive_bayes = BernoulliNB()\n",
    "        #wordvetor erstellung\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        x_train = vectorizer.fit_transform(X_train)\n",
    "        x_test = vectorizer.transform(X_test)\n",
    "        pickle.dump(vectorizer,open(countervector_file,\"wb\"))\n",
    "        #trainieren\n",
    "        naive_bayes.fit(x_train, y_train)\n",
    "        predictions = naive_bayes.predict(x_test)\n",
    "        f = open(self.directoryname+\"/\"+self.directoryname_output+ \"/Precision_Accuracy_Recall_F1_:\"+str(id)+\".txt\",\"w\")\n",
    "        f.write(\"Accuracy score\"+ str (accuracy_score(y_test, predictions))+\"\\n\")\n",
    "        f.write('Precision score: ' + str (precision_score(y_test, predictions))+\"\\n\")\n",
    "        f.write('Recall score: ' + str (recall_score(y_test, predictions))+\"\\n\")\n",
    "        f.write('f1_score score: ' + str (f1_score(y_test, predictions))+\"\\n\")\n",
    "        f.close()\n",
    "        pickle.dump(naive_bayes, open(bayes_file, 'wb'))\n",
    "        \n",
    "    def get_filename(self, id,filepath,name=\"\"):\n",
    "        filename = \"/\" + name +\"-\"+str(id)+\".sav\"\n",
    "        return filepath + filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = empamos_rest_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Stoppwörter/Lemmatisieren Stoppwörter entfernt/Lemmatisiert,Stoppwörter entfernt/Nicht Lemmatisiert,Stoppwörter/Lemmatisiert\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained!1292\n",
      "Keine Daten vorhanden! 1409\n",
      "Keine Daten vorhanden! 1417\n",
      "Keine Daten vorhanden! 1433\n",
      "Keine Daten vorhanden! 1446\n",
      "Keine Daten vorhanden! 1455\n",
      "Keine Daten vorhanden! 1464\n",
      "Keine Daten vorhanden! 1473\n",
      "Keine Daten vorhanden! 1482\n",
      "trained!1492\n",
      "Keine Daten vorhanden! 1515\n",
      "Keine Daten vorhanden! 1665\n",
      "Keine Daten vorhanden! 1756\n",
      "Keine Daten vorhanden! 2884\n",
      "Keine Daten vorhanden! 2894\n",
      "Keine Daten vorhanden! 2907\n",
      "Keine Daten vorhanden! 2968\n",
      "Keine Daten vorhanden! 2981\n",
      "Keine Daten vorhanden! 3085\n",
      "Keine Daten vorhanden! 3098\n",
      "trained!3142\n",
      "trained!3173\n",
      "Keine Daten vorhanden! 3194\n",
      "Keine Daten vorhanden! 3285\n",
      "Keine Daten vorhanden! 3312\n",
      "Keine Daten vorhanden! 3377\n",
      "Keine Daten vorhanden! 3755\n",
      "Keine Daten vorhanden! 4426\n",
      "Keine Daten vorhanden! 5718\n",
      "Keine Daten vorhanden! 5729\n",
      "Keine Daten vorhanden! 5740\n",
      "trained!7090\n",
      "Keine Daten vorhanden! 7098\n",
      "Keine Daten vorhanden! 7106\n",
      "Keine Daten vorhanden! 7115\n",
      "Keine Daten vorhanden! 7123\n",
      "Keine Daten vorhanden! 7227\n",
      "Keine Daten vorhanden! 8520\n",
      "trained!8528\n",
      "trained!8536\n",
      "Keine Daten vorhanden! 8544\n",
      "Keine Daten vorhanden! 8552\n",
      "Keine Daten vorhanden! 8560\n",
      "Keine Daten vorhanden! 8568\n",
      "Keine Daten vorhanden! 8576\n",
      "Keine Daten vorhanden! 8584\n",
      "Keine Daten vorhanden! 8592\n",
      "Keine Daten vorhanden! 8608\n",
      "Keine Daten vorhanden! 8616\n",
      "Keine Daten vorhanden! 8624\n",
      "Keine Daten vorhanden! 8632\n",
      "Keine Daten vorhanden! 8640\n",
      "Keine Daten vorhanden! 8656\n",
      "Keine Daten vorhanden! 8664\n",
      "Keine Daten vorhanden! 8680\n",
      "Keine Daten vorhanden! 8688\n",
      "Keine Daten vorhanden! 8712\n",
      "Keine Daten vorhanden! 8720\n",
      "Keine Daten vorhanden! 8740\n",
      "Keine Daten vorhanden! 8749\n",
      "Keine Daten vorhanden! 8767\n",
      "Keine Daten vorhanden! 23101\n",
      "Keine Daten vorhanden! 23187\n",
      "Keine Daten vorhanden! 23211\n",
      "Keine Daten vorhanden! 26931\n",
      "Keine Daten vorhanden! 26949\n",
      "Keine Daten vorhanden! 33875\n",
      "Keine Daten vorhanden! 33933\n",
      "Keine Daten vorhanden! 34403\n",
      "Keine Daten vorhanden! 34480\n",
      "Keine Daten vorhanden! 34495\n",
      "Keine Daten vorhanden! 35141\n",
      "Keine Daten vorhanden! 35703\n",
      "Keine Daten vorhanden! 38364\n",
      "Keine Daten vorhanden! 39858\n",
      "Keine Daten vorhanden! 66662\n",
      "Keine Daten vorhanden! 67129\n",
      "Keine Daten vorhanden! 73230\n",
      "Keine Daten vorhanden! 73244\n",
      "Keine Daten vorhanden! 86746\n",
      "Keine Daten vorhanden! 86774\n",
      "Keine Daten vorhanden! 86801\n",
      "Keine Daten vorhanden! 86813\n",
      "Keine Daten vorhanden! 101011\n",
      "Keine Daten vorhanden! 112052\n",
      "Keine Daten vorhanden! 112066\n",
      "trained!164466\n",
      "Keine Daten vorhanden! 165619\n",
      "trained!166903\n",
      "Keine Daten vorhanden! 167602\n",
      "Keine Daten vorhanden! 168482\n",
      "Keine Daten vorhanden! 169190\n",
      "Keine Daten vorhanden! 171714\n",
      "Keine Daten vorhanden! 174222\n",
      "Keine Daten vorhanden! 175215\n",
      "Keine Daten vorhanden! 175223\n",
      "Keine Daten vorhanden! 175231\n",
      "Keine Daten vorhanden! 175239\n",
      "Keine Daten vorhanden! 175613\n",
      "Keine Daten vorhanden! 176363\n",
      "Keine Daten vorhanden! 181163\n",
      "Keine Daten vorhanden! 182950\n",
      "Keine Daten vorhanden! 183168\n",
      "Keine Daten vorhanden! 183664\n",
      "Keine Daten vorhanden! 210095\n",
      "Keine Daten vorhanden! 211189\n"
     ]
    }
   ],
   "source": [
    "loc = BayesTrainer()\n",
    "factory = TrainingSetFactory(4)\n",
    "pattern_ids = [pattern['pattern_id'] for pattern in client.get_patterns()]\n",
    "\n",
    "for id in pattern_ids:\n",
    "    df = factory.trainings_dataframe(id) \n",
    "    if (factory.isValid):\n",
    "        loc.directoryname = \"Lemmatized_With_Stopwords\"\n",
    "        #\n",
    "        dfPos = factory.Sentences_to_DF(factory.tokenized_pos_sentences,1)\n",
    "        dfNeg = factory.Sentences_to_DF(factory.tokenized_neg_sentences,0)\n",
    "        #\n",
    "        df = dfPos.append(dfNeg)\n",
    "        loc.train(id, df)\n",
    "        f = open(\"Lemmatized_With_Stopwords/\"+str(id)+\"Sätze.csv\",\"w\")\n",
    "        df.to_csv(f, header=False)\n",
    "        f.close()       \n",
    "        ###\n",
    "        loc.directoryname = \"Bayes_without_stopwords_lemmatized\"\n",
    "        dfPos = factory.Sentences_to_DF(factory.tokenized_stopwords_removed_pos_sentences,1)\n",
    "        dfNeg = factory.Sentences_to_DF(factory.tokenized_stopwords_removed_neg_sentences,0)\n",
    "        #\n",
    "        df = dfPos.append(dfNeg)     \n",
    "        loc.train(id, df)\n",
    "        f = open(\"Bayes_without_stopwords_lemmatized/\"+str(id)+\"Sätze.csv\",\"w\")\n",
    "        df.to_csv(f, header=False)\n",
    "        f.close()          \n",
    "        ### \n",
    "        loc.directoryname = \"Bayes_no_lemmatize_with_stopwords\"\n",
    "        dfPos = factory.Sentences_to_DF(factory.no_tokenized_with_stopwords_pos_sentences,1)\n",
    "        dfNeg = factory.Sentences_to_DF(factory.no_tokenized_with_stopwords_neg_sentences,0)\n",
    "        #\n",
    "        df = dfPos.append(dfNeg)\n",
    "        # \n",
    "        loc.train(id, df)\n",
    "        f = open(\"Bayes_no_lemmatize_with_stopwords/\"+str(id)+\"Sätze.csv\",\"w\")\n",
    "        df.to_csv(f, header=False)\n",
    "        f.close()      \n",
    "        #\n",
    "        loc.directoryname = \"Bayes_no_lemmatize_stopword_removed\"\n",
    "        #\n",
    "        dfPos = factory.Sentences_to_DF(factory.no_tokenized_removed_stopwords_pos_sentences,1)\n",
    "        dfNeg = factory.Sentences_to_DF(factory.no_tokenized_removed_stopwords_neg_sentences,0)\n",
    "        #\n",
    "        df = dfPos.append(dfNeg)     \n",
    "        loc.train(id, df)  \n",
    "        f = open(\"Bayes_no_lemmatize_stopword_removed/\"+str(id)+\"Sätze.csv\",\"w\")\n",
    "        df.to_csv(f, header=False)\n",
    "        f.close()  \n",
    "        print(\"trained!\"+str(id))\n",
    "        \n",
    "    else:\n",
    "        print(f'Keine Daten vorhanden! {id}')\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
