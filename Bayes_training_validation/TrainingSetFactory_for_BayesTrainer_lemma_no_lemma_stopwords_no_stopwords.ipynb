{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font size=\"6\" weight=\"bold\">Generieren der Testdaten</font>__\n",
    "\n",
    "Hier werden die Trainingsdaten für den Classificator erstellt. <br>\n",
    "Als positive Sätze werden für jedes Pattern die empirisch gefundenen Sätze verwendet. <br>\n",
    "Für die negativen Sätze werden, über die Schnittstelle gefundene, Anleitungen ohne das jeweilige Pattern verwendet.\n",
    "<br>\n",
    "<br>\n",
    "__TODO__: Umbauen für generische Verwendung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einbinden der Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra.cluster import Cluster\n",
    "from spacy.lang.de import German # updated\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest Client erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run REST.ipynb\n",
    "client = empamos_rest_client()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbindung zur Datenbank aufbauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = PlainTextAuthProvider(username='itp_mining', password='mining-data')\n",
    "cluster = Cluster(['big1.informatik.fh-nuernberg.de'], auth_provider=ap)\n",
    "session = cluster.connect('itp_mining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse TrainigsSetFactory\n",
    "\n",
    "__Funktionen:__\n",
    "\n",
    "__init__(threshold): Initialisierung. Als Parameter kann ein Threshold übergeben werden. Dieser bestimmt aus wie vielen Wörtern ein Satz mindestens bestehen muss. Sätze mit weniger Wörtern werden ignoriert. <br><br>\n",
    "__trainings_dataframe(pattern_id, isBalanced,has_Stopwords)__: Erstellt ein Trainingsset. Die Parameter sind:  <br>\n",
    "pattern_id is vom Datenyp Integer. Bestimmt, zu welchem Pattern ein Dataframe erstellt werden soll.<br> \n",
    "is_balanced ist vom Datentyp Boolean. Bestimmt, ob ein Panda Dataframe aus gleich vielen positiven oder negativen Sätzen bestehen soll.   <br>\n",
    "stopwords_removed ist vom Datentyp Boolean. Bestimmt, ob Stoppwörter entfernt werden sollen.<br>\n",
    "return type\n",
    "<br><br>\n",
    "__Sentences_to_DF(sentences, isPositive)__: \n",
    "<br>Parameter:<br>\n",
    "sentences: String Liste<br>\n",
    "isPositive: Boolean<br>\n",
    "Kovertiert eine String Liste in ein Dataframe um. <br>\n",
    "Return: Panda Dataframe<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Get_ALL_PositiveSentences(pattern_id)__: \n",
    "<br>Parameter:<br>\n",
    "pattern_id: Integer\n",
    "liest alle pattern_evidence_sentences aus der Datenbank aus und Tokenisiert diese anschließen mittels spacy.<br>\n",
    "Return: String Liste<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Get_ALL_NegativeSentences(pattern_id)__: \n",
    "<br>Parameter:<br>\n",
    "pattern_id: Integer\n",
    "liest alle Anleitungen aus der Datenbank, bei denen das Pattern sicher nicht vorkommt und Tokenisiert diese anschließen mittels spacy.<br>\n",
    "Return: String Liste <strong>oder</strong> -1 wenn für das Pattern keine Anleitungen existieren<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Convert_evidence_pattern_search_to_game_manualtexts(evidence_json)__:\n",
    "<br>Parameter:<br>\n",
    "evidence_json: JSON Liste in der Form:<br>\n",
    "'evidence_id': 46869, 'evidence_name': 'DIE GUTEN UND DIE BÖSEN GEISTER', 'solr_id': '856d7bfb42220f7e2a8f8b318d07f874'}\n",
    "Da das erste zurückgegeben JSON in der Datenbank jedoch immer in dieser Form vorliegt:<br>\n",
    "{'hasallpatterns': None, 'hasnopatterns': None, 'results': 45}<br>\n",
    "Wird in der Funktion das immer das erste Element gelöscht.\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Tokenize(game_manuals_text)__:\n",
    "Parameter:<br>\n",
    "game_manuals_text: String Liste.\n",
    "Return: String Liste<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__remove_stopwords(sentences)__:\n",
    "Parameter:<br>\n",
    "sentences: String Liste.<br>\n",
    "Die Funktion entfernt Stoppwörter, wenn die File \"Stopwords.txt\" im Ordner \"Stopwords\" existiert, wird aus diesen Wörtern eine Stopwörter set erzeut, andernfalls wird die Liste von nltk.corpus genutzt.\n",
    "Return: String Liste<br> \n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__SaveSentencesInFile(dataframe,msg)__:\n",
    "Parameter:<br>\n",
    "dataframe: Panda Dataframe<br>\n",
    "msg: String, default wert ist leerer String<br>\n",
    "Diese Funktion speichert die positiven und negativen Sätze in ein Textfile \"TestSentences.csv\" ab.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TrainingSetFactory:\n",
    "    isValid = 1;\n",
    "    treshold = int\n",
    "   \n",
    "    def __init__(self,treshold=0):\n",
    "        self.client = empamos_rest_client()\n",
    "        self.treshold = treshold\n",
    "        \n",
    "    def trainings_dataframe(self,pattern_id, isBalanced=True):\n",
    "        pos_sentences = self.Get_ALL_PositiveSentences(pattern_id)\n",
    "        neg_sentences = self.Get_ALL_NegativeSentences(pattern_id)\n",
    "        \n",
    "        if neg_sentences == -1:\n",
    "            self.isValid = 0\n",
    "            return pd.DataFrame()\n",
    "        else:\n",
    "            self.isValid = 1\n",
    "\n",
    "        # the negatives are manuals we have to use spacy in order to get sentences out of it\n",
    "        self.balanced_sentences_pos = self.Tokenize_no_lemmas(pos_sentences)\n",
    "        self.balanced_neg_sentences = self.Tokenize_no_lemmas(neg_sentences)\n",
    "\n",
    "        if isBalanced:\n",
    "            if len(self.balanced_sentences_pos) > len(self.balanced_neg_sentences):\n",
    "                self.balanced_sentences_pos = random.sample(self.balanced_sentences_pos,len(self.balanced_neg_sentences))\n",
    "            else:\n",
    "                self.balanced_neg_sentences = random.sample(self.balanced_neg_sentences,len(self.balanced_sentences_pos))    \n",
    "\n",
    "        \n",
    "        #now we lemmatize   tokenized means lemmatized in this context.  \n",
    "        # Tokenized no stopwords removed\n",
    "        self.tokenized_pos_sentences = self.Tokenize(self.balanced_sentences_pos)\n",
    "        self.tokenized_neg_sentences = self.Tokenize(self.balanced_neg_sentences)\n",
    "        ### stopwords removed AND Tokenized\n",
    "        self.tokenized_stopwords_removed_pos_sentences = self.remove_stopwords(self.tokenized_pos_sentences)\n",
    "        self.tokenized_stopwords_removed_neg_sentences = self.remove_stopwords(self.tokenized_neg_sentences)        \n",
    "        ### no lemmatized with stopwords\n",
    "        self.no_tokenized_with_stopwords_pos_sentences = self.balanced_sentences_pos\n",
    "        self.no_tokenized_with_stopwords_neg_sentences = self.balanced_neg_sentences\n",
    "        ### no lemmatized stopwords removed\n",
    "        self.no_tokenized_removed_stopwords_pos_sentences = self.remove_stopwords(self.no_tokenized_with_stopwords_pos_sentences)\n",
    "        self.no_tokenized_removed_stopwords_neg_sentences = self.remove_stopwords(self.no_tokenized_with_stopwords_neg_sentences)\n",
    "        \n",
    "        return 1\n",
    "  \n",
    "    def Sentences_to_DF(self,sentences, isPositive):\n",
    "        df = pd.DataFrame([sent for sent in sentences])\n",
    "        df[1] = [int (isPositive) for sent in sentences]\n",
    "        return df\n",
    "        \n",
    "    def Get_ALL_PositiveSentences(self,pattern_id):\n",
    "        positive_sentences = self.client.get_pattern_evidence_sentences(pattern_id)\n",
    "        return positive_sentences\n",
    "    \n",
    "    def Get_ALL_NegativeSentences(self,pattern_id):\n",
    "        games_without_pattern = self.client.evidence_pattern_search('','',pattern_id,'')\n",
    "        game_text = self.Convert_evidence_pattern_search_to_game_manualtexts(games_without_pattern)  \n",
    "        if game_text == -1:\n",
    "            return -1\n",
    "        return game_text\n",
    "    \n",
    "    def Convert_evidence_pattern_search_to_game_manualtexts(self,evidence_json):\n",
    "        #The first json differs \n",
    "        del evidence_json[0]\n",
    "        if(len(evidence_json) == 0):\n",
    "            return -1\n",
    "        manuals = []\n",
    "        for evidence in evidence_json:\n",
    "            result = session.execute(\"select text from working_text where SourceUID = '\" + str(evidence[\"solr_id\"]) + \"';\")\n",
    "            text = \"\"\n",
    "            for line in result:\n",
    "                text+=line.text\n",
    "            if len(text.strip()) > 0:\n",
    "                manuals.append(text) \n",
    "        return manuals\n",
    "    \n",
    "    def Tokenize(self,game_manuals_text):\n",
    "        tokenized_sentences = []\n",
    "        nlp = spacy.load('de')\n",
    "        for text in game_manuals_text:\n",
    "            spacy_doc = nlp(text)\n",
    "            for sentence in spacy_doc.sents:\n",
    "                if(self.treshold <= len(sentence)):\n",
    "                    tokenized_sentences.append(str(sentence.lemma_))\n",
    "        return tokenized_sentences\n",
    "    \n",
    "    def Tokenize_no_lemmas(self,game_manuals_text):\n",
    "        tokenized_sentences = []\n",
    "        nlp = spacy.load('de')\n",
    "        for text in game_manuals_text:\n",
    "            spacy_doc = nlp(text)\n",
    "            for sentence in spacy_doc.sents:\n",
    "                if(self.treshold <= len(sentence)):\n",
    "                    tokenized_sentences.append(str(sentence.text))\n",
    "        return tokenized_sentences\n",
    "\n",
    "    \n",
    "    def remove_stopwords(self,sentences):\n",
    "        sentence_filtert =[]\n",
    "        if(os.path.isfile(\"Stopwords/Stopword.txt\")):\n",
    "            STOPWORDS = set(line.strip() for line in open('Stopwords/Stopwords.txt'))\n",
    "        else:\n",
    "            STOPWORDS = set(stopwords.words('german'))\n",
    "        for text in sentences:\n",
    "            tmp=\"\"\n",
    "            filtert = [w.lower() for w in text.split(\" \") if not w.lower() in STOPWORDS]\n",
    "            for word in filtert:\n",
    "                tmp+=word+\" \"\n",
    "            sentence_filtert.append(tmp.rstrip())\n",
    "        return sentence_filtert\n",
    "        \n",
    "    def SaveSentencesInFile(self,dataframe):\n",
    "        f = open(\"TestSentences.csv\",\"a+\")\n",
    "        dataframe.to_csv(f, header=False)\n",
    "        f.write(\"\\nSentences were selected on:\" + datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\\n\")\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
