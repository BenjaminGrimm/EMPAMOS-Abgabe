{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font size=\"6\" weight=\"bold\">SVM Localizer</font>__\n",
    "\n",
    "Diese Klasse beinhaltet eine Liste mit den trainierten Modellen für die einzelnen Pattern. <br>\n",
    "Bei der Initialisierung werden alle Modelle neu erstellt oder geladen, falls sie bereits vorhanden sind <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einbinden der Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import os \n",
    "import os.path as path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rest Client erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./REST.ipynb\n",
    "client = empamos_rest_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_Pattern Klasse laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./GamePatterns.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp_processed_game Klasse laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nlpProcessedGame.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse SVMLocalizer\n",
    "\n",
    "__Parameter:__ <br>\n",
    "__directoryname__: Name des Verzeichnis der Modelle <br>\n",
    "__minSentenceLength__: Mindest Anzahl der Wörter / Satz <br>\n",
    "__trueValue__: Gibt an welchen Wert der Klassifikator ausgibt, wenn ein Satz das gesuchte Pattern enthält (0 bei Matthias Kriegbaum Modellen | 1 bei unseren Modellen)  <br>\n",
    "__remove_stopwords__: Gibt an ob Stopwörter entfernt werden soll (True = ja | False = nein)\n",
    "\n",
    "\n",
    "__Funktionen:__\n",
    "\n",
    "__ctor__: Initialisieren <br>\n",
    "__load_all_models(filepath)__: Läd alle trainierten Modelle. Filepath ist optional. Ist keiner angegeben, werden die Modelle aus dem Verzeichnis mit dem Namen directoryname geladen <br>\n",
    "__load_model(id, filepath)__: Läd trainiertes Model über id und filepath. Filepath ist optional <br>\n",
    "__train(id, dataframe)__: Trainiert Model mit übergebenem Dataframe und speichert unter der angegebenen id <br>\n",
    "__read_nlp_processed_game_to_game_patterns(processedManual)__: NLP-verarbeitete Anleitung als Übergabeparamter. Gibt Game_Pattern für Anleitung zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMLocalizer: \n",
    "    modelDictionary = {}\n",
    "    directoryname=\"SVM_Models\"\n",
    "    minSentenceLength = 3\n",
    "    trueValue = 1\n",
    "    remove_stopwords = True\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.modelDictionary = {}\n",
    "                       \n",
    "    \n",
    "    def load_all_models(self, filepath = ''):\n",
    "        if filepath == '':\n",
    "            filepath = f'{os.getcwd()}/{self.directoryname}'\n",
    "        \n",
    "        for file in os.listdir(filepath):\n",
    "            if file.endswith(\".pkl\"):\n",
    "                pattern_id = str(file).replace('Pattern-', '').replace('.pkl', '')\n",
    "                self.load_model(pattern_id, filepath)\n",
    "                   \n",
    "    \n",
    "    def load_model(self, id, filepath):\n",
    "        file = self.get_filename(filepath, id)\n",
    "        \n",
    "        if path.exists(file):\n",
    "            try:\n",
    "                self.modelDictionary[id] = load(file) \n",
    "            except:\n",
    "                print('Fehler beim Dateizugriff. Prüfen Sie die Eigenschaft <directoryName>! ')\n",
    "        \n",
    "    def train(self, id, dataframe):\n",
    "        folder = f'{os.getcwd()}/{self.directoryname}'\n",
    "        \n",
    "        file = self.get_filename(folder, id)\n",
    "        os.makedirs(self.directoryname,exist_ok=True)\n",
    "        os.makedirs(self.directoryname+\"/SVM_Output\", exist_ok=True)\n",
    "        \n",
    "        X = dataframe[0]\n",
    "        y = dataframe[1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 42)\n",
    "        \n",
    "        ## SGDClassifier() zum erstellen des Models\n",
    "        #sgd = Pipeline([('vect', CountVectorizer()),\n",
    "        #('tfidf', TfidfTransformer()),\n",
    "        #('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "        #])\n",
    "        #\n",
    "        #sgd.fit(X_train, y_train)\n",
    "        \n",
    "        lsvc = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LinearSVC(random_state=None)),\n",
    "               ])\n",
    "\n",
    "        parameters = {\n",
    "             \"vect__ngram_range\": [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "             \"tfidf__use_idf\": (True, False),\n",
    "             \"clf__loss\" : ['hinge', 'squared_hinge'],\n",
    "             \"clf__tol\" : [1e-6, 1e-5, 1e-4]\n",
    "        }\n",
    "\n",
    "        ##Hier zu betrachtendes Attribut über scoring = <attribute> wählen\n",
    "        gs_clf = GridSearchCV(lsvc, parameters, cv=5, n_jobs=-1, scoring = 'accuracy')\n",
    "\n",
    "        gs_clf = gs_clf.fit(X_train, y_train)\n",
    "        \n",
    "        predictions = gs_clf.predict(X_test)\n",
    "        \n",
    "        \n",
    "        f = open(self.directoryname+\"/SVM_Output/Precision_Accuracy_Recall_F1_\"+str(id)+\".txt\",\"w\")\n",
    "        f.write(\"Accuracy score\"+ str (accuracy_score(y_test, predictions))+\"\\n\")\n",
    "        f.write('Precision score: ' + str (precision_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "        f.write('Recall score: ' + str (recall_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "        f.write('f1_score score: ' + str (f1_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "        f.close()\n",
    "        \n",
    "        dump(gs_clf, open(file, 'wb'))\n",
    "    \n",
    "        \n",
    "    def create_sentence_index_list(self, prediction, sentences):\n",
    "        sentenceIndexList = [] \n",
    "        count = 0\n",
    "        for i in prediction:\n",
    "            if i == self.trueValue and len(sentences[count].split()) >= self.minSentenceLength:\n",
    "                sentenceIndexList.append(count)\n",
    "        \n",
    "            count += 1\n",
    "        \n",
    "        return sentenceIndexList\n",
    "    \n",
    "    \n",
    "    def get_filename(self, filepath, id):\n",
    "        filename = f'/Pattern-{str(id)}.pkl'\n",
    "        return filepath + \"/\" + filename\n",
    "    \n",
    "    \n",
    "    def remove_stopwords(self, sentence):\n",
    "        if(os.path.isfile(\"Stopwords/Stopword.txt\")):\n",
    "            STOPWORDS = set(line.strip() for line in open('Stopwords/Stopwords.txt'))\n",
    "        else:\n",
    "            STOPWORDS = set(stopwords.words('german'))\n",
    "            \n",
    "        sentence =  [word.lower() for word in sentence.split() if not word.lower() in STOPWORDS] \n",
    "        sentence_clean =\"\"\n",
    "            \n",
    "        for word in sentence:\n",
    "            sentence_clean+=word+\" \"\n",
    "        return sentence_clean\n",
    "        \n",
    "    \n",
    "    def read_nlp_processed_game_to_game_patterns(self, processedManual):\n",
    "        doc=processedManual.get_spacy_doc()\n",
    "        gamePattern = game_patterns()\n",
    "        gamePattern.set_ID(processedManual.get_ID())\n",
    "\n",
    "        sentenceTexts = []\n",
    "        for sent in doc.sents:\n",
    "            sentenceTexts.append(str(sent))\n",
    "            \n",
    "        gamePattern.set_sentences(sentenceTexts)\n",
    "        \n",
    "        listLemmaSentences = list(str(sent.lemma_) for sent in doc.sents)\n",
    "        \n",
    "        if(len(listLemmaSentences) == 0):\n",
    "            return gamePattern\n",
    "        \n",
    "        if self.remove_stopwords:\n",
    "            listLemmaSentences = [self.remove_stopwords(sent) for sent in listLemmaSentences]\n",
    "\n",
    "        for key in self.modelDictionary:\n",
    "            model = self.modelDictionary.get(key)\n",
    "            if model:\n",
    "                prediction = model.predict(listLemmaSentences)\n",
    "                indexedSentences = self.create_sentence_index_list(prediction, listLemmaSentences)\n",
    "                gamePattern.add_pattern(int(key),indexedSentences)\n",
    "                \n",
    "        return gamePattern\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
