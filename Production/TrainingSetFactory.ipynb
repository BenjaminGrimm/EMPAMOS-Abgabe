{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font size=\"6\" weight=\"bold\">Generieren der Testdaten</font>__\n",
    "\n",
    "Hier werden die Trainingsdaten für den Classificator erstellt. <br>\n",
    "Als positive Sätze werden für jedes Pattern die empirisch gefundenen Sätze verwendet. <br>\n",
    "Für die negativen Sätze werden über die Schnittstelle gefundene Anleitungen bei dem das jeweilige Pattern definitiv nicht vorkommt verwendet.\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einbinden der Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra.cluster import Cluster\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbindung zur Datenbank aufbauen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = PlainTextAuthProvider(username='itp_mining', password='mining-data')\n",
    "cluster = Cluster(['big1.informatik.fh-nuernberg.de'], auth_provider=ap)\n",
    "session = cluster.connect('itp_mining')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REST Notebok ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run REST.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse TrainigsSetFactory\n",
    "\n",
    "__Funktionen:__\n",
    "\n",
    "__init__(threshold): Initialisierung. Als Parameter kann ein Threshold übergeben werden. Dieser bestimmt aus wie vielen Wörtern ein Satz mindestens bestehen muss. Sätze mit weniger Wörtern werden ignoriert. <br><br>\n",
    "__trainings_dataframe(pattern_id, isBalanced,has_Stopwords)__: Erstellt ein Trainingsset. <br>Parameter:<br>\n",
    "pattern_id is vom Datenyp Integer. Bestimmt, zu welchem Pattern ein Dataframe erstellt werden soll.<br> \n",
    "is_balanced ist vom Datentyp Boolean. Bestimmt, ob ein Panda Dataframe aus gleich vielen positiven oder negativen Sätzen bestehen soll.   <br>\n",
    "stopwords_removed ist vom Datentyp Boolean. Bestimmt, ob Stoppwörter entfernt werden sollen.<br>\n",
    "Return:<br>\n",
    "Panda Dataframe\n",
    "<br><br>\n",
    "__Sentences_to_DF(sentences, isPositive)__: \n",
    "<br>Parameter:<br>\n",
    "sentences: String Liste<br>\n",
    "isPositive: Boolean<br>\n",
    "Return:<br> Panda Dataframe<br>\n",
    "Kovertiert eine String Liste in ein Dataframe um. <br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Get_ALL_PositiveSentences(pattern_id)__: \n",
    "<br>Parameter:<br>\n",
    "pattern_id: Integer<br>\n",
    "Return:<br> String Liste<br>\n",
    "liest alle pattern_evidence_sentences aus der Datenbank aus und Tokenisiert diese anschließen mittels spacy.<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Get_ALL_NegativeSentences(pattern_id)__: \n",
    "<br>Parameter:<br>\n",
    "pattern_id: Integer\n",
    "Return:<br> String Liste <strong>oder</strong> -1 wenn für das Pattern keine Anleitungen existieren<br>\n",
    "liest alle Anleitungen aus der Datenbank, bei denen das Pattern sicher nicht vorkommt und Tokenisiert diese anschließen mittels spacy.<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Convert_evidence_pattern_search_to_game_manualtexts(evidence_json)__:\n",
    "<br>Parameter:<br>\n",
    "evidence_json: JSON Liste in der Form:<br>\n",
    "'evidence_id': 46869, 'evidence_name': 'DIE GUTEN UND DIE BÖSEN GEISTER', 'solr_id': '856d7bfb42220f7e2a8f8b318d07f874'}\n",
    "Da das erste zurückgegeben JSON in der Datenbank jedoch immer in dieser Form vorliegt:<br>\n",
    "{'hasallpatterns': None, 'hasnopatterns': None, 'results': 45}<br>\n",
    "Wird in der Funktion das immer das erste Element gelöscht.<br>\n",
    "Return:<br>\n",
    "String List<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__Tokenize(game_manuals_text)__:\n",
    "<br>Parameter:<br>\n",
    "game_manuals_text: String Liste.<br>\n",
    "Return:<br> String Liste<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__remove_stopwords(sentences)__:\n",
    "<br>Parameter:<br>\n",
    "sentences: String Liste.<br>\n",
    "Return:<br> String Liste<br> \n",
    "Die Funktion entfernt Stoppwörter, wenn die File \"Stopwords.txt\" im Ordner \"Stopwords\" existiert, wird aus diesen Wörtern eine Stopwörter set erzeut, andernfalls wird die Liste von nltk.corpus genutzt.<br>\n",
    "Diese Funktion ist eine interne Funktion und soll nicht aufgerufen werden <br><br>\n",
    "__SaveSentencesInFile(dataframe)__:\n",
    "<br>Parameter:<br>\n",
    "dataframe: Panda Dataframe<br>\n",
    "Diese Funktion speichert die positiven und negativen Sätze in ein Textfile \"TestSentences.csv\" ab.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TrainingSetFactory:\n",
    "    isValid = 0;\n",
    "    treshold = int\n",
    "   \n",
    "    def __init__(self,treshold=0):\n",
    "        self.client = empamos_rest_client()\n",
    "        self.treshold = treshold\n",
    "        \n",
    "    def trainings_dataframe(self,pattern_id, isBalanced=True,stopwords_removed=False):\n",
    "        pos_sentences = self.Get_ALL_PositiveSentences(pattern_id)\n",
    "        neg_sentences = self.Get_ALL_NegativeSentences(pattern_id)\n",
    "        \n",
    "        if neg_sentences == -1:\n",
    "            self.isValid = 0\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        if isBalanced:\n",
    "            if len(pos_sentences) > len(neg_sentences):\n",
    "                pos_sentences = random.sample(pos_sentences,len(neg_sentences))\n",
    "            else:\n",
    "                neg_sentences = random.sample(neg_sentences,len(pos_sentences))\n",
    "        if stopwords_removed:\n",
    "            pos_sentences = self.remove_stopwords(pos_sentences)\n",
    "            neg_sentences = self.remove_stopwords(neg_sentences)\n",
    "        dfPos = self.Sentences_to_DF(pos_sentences,True)\n",
    "        dfNeg = self.Sentences_to_DF(neg_sentences,False)\n",
    "        self.SaveSentencesInFile(dfPos.append(dfNeg))\n",
    "        self.isValid = 1\n",
    "        return dfPos.append(dfNeg)\n",
    "  \n",
    "    def Sentences_to_DF(self,sentences, isPositive):\n",
    "        df = pd.DataFrame([sent for sent in sentences])\n",
    "        df[1] = [int (isPositive) for sent in sentences]\n",
    "        return df\n",
    "        \n",
    "    def Get_ALL_PositiveSentences(self,pattern_id):\n",
    "        positive_sentences = self.client.get_pattern_evidence_sentences(pattern_id)\n",
    "        tokenized_sentences = self.Tokenize(positive_sentences)\n",
    "        return tokenized_sentences\n",
    "    \n",
    "    def Get_ALL_NegativeSentences(self,pattern_id):\n",
    "        games_without_pattern = self.client.evidence_pattern_search('','',pattern_id,'')\n",
    "        game_text = self.Convert_evidence_pattern_search_to_game_manualtexts(games_without_pattern)  \n",
    "        if game_text == -1:\n",
    "            return -1\n",
    "        tokenized_manuals_sentences = self.Tokenize(game_text)\n",
    "        return tokenized_manuals_sentences\n",
    "    \n",
    "    def Convert_evidence_pattern_search_to_game_manualtexts(self,evidence_json):\n",
    "        #The first json differs \n",
    "        del evidence_json[0]\n",
    "        if(len(evidence_json) == 0):\n",
    "            return -1\n",
    "        manuals = []\n",
    "        for evidence in evidence_json:\n",
    "            result = session.execute(\"select text from working_text where SourceUID = '\" + str(evidence[\"solr_id\"]) + \"';\")\n",
    "            text = \"\"\n",
    "            for line in result:\n",
    "                text+=line.text\n",
    "            if len(text.strip()) > 0:\n",
    "                manuals.append(text) \n",
    "        return manuals\n",
    "    \n",
    "    def Tokenize(self,game_manuals_text):\n",
    "        tokenized_sentences = []\n",
    "        tokenized_treshold_sentences = []\n",
    "        nlp = spacy.load('de')\n",
    "        for text in game_manuals_text:\n",
    "            spacy_doc = nlp(text)\n",
    "            for sentence in spacy_doc.sents:\n",
    "                if(self.treshold <= len(sentence)):\n",
    "                    tokenized_sentences.append(sentence.lemma_)\n",
    "        return tokenized_sentences\n",
    "    \n",
    "    def remove_stopwords(self,sentences):\n",
    "        sentence_filtert =[]\n",
    "        if(os.path.isfile(\"Stopwords/Stopword.txt\")):\n",
    "            STOPWORDS = set(line.strip() for line in open('Stopwords/Stopwords.txt'))\n",
    "        else:\n",
    "            STOPWORDS = set(stopwords.words('german'))\n",
    "        for text in sentences:\n",
    "            tmp=\"\"\n",
    "            filtert = [w.lower() for w in text.split(\" \") if not w.lower() in STOPWORDS]\n",
    "            for word in filtert:\n",
    "                tmp+=word+\" \"\n",
    "            sentence_filtert.append(tmp.rstrip())\n",
    "        return sentence_filtert\n",
    "        \n",
    "    def SaveSentencesInFile(self,dataframe):\n",
    "        f = open(\"TestSentences.csv\",\"a+\")\n",
    "        dataframe.to_csv(f, header=False)\n",
    "        f.write(\"\\nSentences were selected on:\" + datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\") + \"\\n\\n\")\n",
    "        f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
