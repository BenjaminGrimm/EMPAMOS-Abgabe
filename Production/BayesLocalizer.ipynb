{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__<font size=\"6\" weight=\"bold\">Bayes Localizer</font>__\n",
    "\n",
    "Diese Klasse beinhaltet eine Liste mit den trainierten Modellen für die einzelnen Pattern. <br>\n",
    "Bei der Initialisierung werden alle Modelle neu erstellt oder geladen, falls sie bereits vorhanden sind <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einbinden der Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import os \n",
    "import os.path as path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Game_Pattern Klasse laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./GamePatterns.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nlp_processed_game Klasse laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./nlpProcessedGame.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasse Bayes Localizer\n",
    "\n",
    "__Klassenmember__:<br>\n",
    "directoryname: Name des Verzeichnis der Modelle<br>\n",
    "directoryname_output: Name des Verzeichnis des Outputs bei der train function.<br>\n",
    "min_length: Mindest Anzahl der Wörter / Satz<br>\n",
    "remove_stopwords: Gibt an ob Stopwörter entfernt werden soll (True = ja | False = nein), wenn eine neue Anleitung predicted werden soll.<br>\n",
    "lemmatize: Gibt an ob Sätze lemmatisiert werden sollen, wenn eine neue Anleitung predicted werden soll.<br>\n",
    "__Funktionen:__\n",
    "\n",
    "__init__: \n",
    "<br>Parameter:<br>\n",
    "remove_stopwords : Boolean<br>\n",
    "lemmatize : Boolean<br>\n",
    "Initialisierung, es wird ein leeres modelDictionary und counterVectorizeDirectory erzeugt.\n",
    "__set_min_sentence_length(min_length)__:\n",
    "<br>Parameter<br>\n",
    "min_length : integer<br>\n",
    "Mindest-Wortanzahl der Sätze. Sätze die weniger Wörter beinhalten, werden ignoriert.<br><br>\n",
    "__load_all_models(filepath)__: \n",
    "<br>Parameter<br>\n",
    "filepath : String (Default ist leerer String)\n",
    "Lädt alle trainierten Modelle. Filepath ist optional. Ist keiner angegeben, werden die Modelle aus dem Standardverzeichnis(directoryname) geladen<br><br>\n",
    "__load_model(id,filepath)__: \n",
    "<br>Parameter<br>\n",
    "id : integer <br>\n",
    "filepath : String <br>\n",
    "Läd trainiertes Model über id und filepath.\n",
    "<br><br>\n",
    "__load_counterVectorize(id,filepath)__:\n",
    "<br>Parameter<br>\n",
    "id : integer <br>\n",
    "filepath: String <br>\n",
    "Läd die Sparse Matrix über id und filepath.\n",
    "<br><br>\n",
    "__train(id, dataframe)__:\n",
    "<br>Parameter<br>\n",
    "id : integer<br>\n",
    "dataframe: panda dataframe<br>\n",
    "Trainiert Model mit übergebenen Dataframe und speichert unter speichert das Model im Standardverzeichnis ab \"directoryname\".Ein weiteres File, welches Accuracy,Precision,Recall und F1 Score des trainierten Modell beinhaltet wird im \"directoryname\"/Bayes_Output abgespeichert.<br>\n",
    "<br><br>\n",
    "__clean_stopwords(sentences)__:\n",
    "<br> Parameter <br>\n",
    "sentences: String<br>\n",
    "Entfernt Stoppwörter aus Anleitungen die predicted werden sollen, sofern der \"remove_stopwords\" Parameter auf True ist.<br>\n",
    "<br><br>\n",
    "__create_sentence_index_list(predictions,sentences)__:\n",
    "<br>Parameter<br>\n",
    "predictions: list of integers<br>\n",
    "sentences : String list\n",
    "Gibt eine Liste mit den Satznummern zurück, in denen das Pattern gefunden wurde. <br><br>\n",
    "__read_nlp_processed_game_to_game_patterns(processedManual)__: NLP-verarbeitete Anleitung als Übergabeparamter. Gibt Game_Pattern für Anleitung zurück."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesLocalizer: \n",
    "    directoryname=\"Bayes\"\n",
    "    directoryname_output =\"Bayes_Output\"\n",
    "    \n",
    "    def __init__(self,remove_stopwords=False,lemmatize = False):\n",
    "        self.modelDictionary = {}\n",
    "        self.counterVectorizeDirectory ={}\n",
    "        self.min_length = 0\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        \n",
    "    def set_min_sentence_length(self, min_length):\n",
    "        self.min_length = min_length\n",
    "                       \n",
    "    def load_all_models(self,filepath=\"\"):\n",
    "        if filepath == '':\n",
    "            filepath = f'{os.getcwd()}/{self.directoryname}'\n",
    "        if os.path.exists(filepath):\n",
    "            for file in os.listdir(filepath):\n",
    "                if file.endswith(\".sav\") and file.startswith(\"B\"):\n",
    "                    pattern_id = str(file).replace('Bayes-', '').replace(\"Counter-\",'').replace('.sav', '')\n",
    "                    self.modelDictionary[pattern_id] = self.load_model(pattern_id, filepath)\n",
    "                    self.counterVectorizeDirectory[pattern_id] = self.load_counterVectorize(pattern_id, filepath)\n",
    "        else:\n",
    "            print(\"Directory does not exist!\")\n",
    "\n",
    "    \n",
    "    def load_model(self, id,filepath):\n",
    "        file = self.get_filename(id,filepath,\"Bayes\")\n",
    "        if path.exists(file):\n",
    "            return pickle.load(open(file, 'rb'))\n",
    "        else:\n",
    "            print(\"Warning:No Bayes Model found for the Pattern:\"+str(id))\n",
    "            return None\n",
    "    \n",
    "    def load_counterVectorize(self,id,filepath):\n",
    "        file = self.get_filename(id,filepath,\"Counter\")\n",
    "        if path.exists(file):\n",
    "            #TODO try except mit Meldung\n",
    "            return pickle.load(open(file, 'rb'))\n",
    "        else:\n",
    "            print(\"Warning: No Sparse Matrix found for the Pattern:\"+str(id))\n",
    "            #TODO Meldung!\n",
    "            return None\n",
    "        \n",
    "    def train(self, id, dataframe):        \n",
    "        #create directory if not exist\n",
    "        os.makedirs(self.directoryname, exist_ok=True)\n",
    "        os.makedirs(self.directoryname+\"/\" +self.directoryname_output, exist_ok=True)\n",
    "        bayes_file = self.get_filename(id,self.directoryname,\"Bayes\")\n",
    "        countervector_file = self.get_filename(id,self.directoryname,\"Counter\")\n",
    "        X = dataframe[0]\n",
    "        Y = dataframe[1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.9 )\n",
    "        naive_bayes = BernoulliNB()\n",
    "        #wordvetor erstellung\n",
    "        vectorizer = CountVectorizer(binary=True)\n",
    "        x_train = vectorizer.fit_transform(X_train)\n",
    "        x_test = vectorizer.transform(X_test)\n",
    "        pickle.dump(vectorizer,open(countervector_file,\"wb\"))\n",
    "        #trainieren\n",
    "        naive_bayes.fit(x_train, y_train)\n",
    "        predictions = naive_bayes.predict(x_test)\n",
    "        f = open(self.directoryname+\"/\"+self.directoryname_output+ \"/Precision_Accuracy_Recall_F1_:\"+str(id)+\".txt\",\"w\")\n",
    "        f.write(\"Accuracy score\"+ str (accuracy_score(y_test, predictions))+\"\\n\")\n",
    "        f.write('Precision score: ' + str (precision_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "        f.write('Recall score: ' + str (recall_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "        f.write('f1_score score: ' + str (f1_score(y_test, predictions,pos_label=0))+\"\\n\")\n",
    "        f.close()\n",
    "        pickle.dump(naive_bayes, open(bayes_file, 'wb'))\n",
    "    \n",
    "    \n",
    "    def get_filename(self, id,filepath,name=\"\"):\n",
    "        filename = \"/\" + name +\"-\"+str(id)+\".sav\"\n",
    "        return filepath + filename\n",
    "    \n",
    "    def clean_stopwords(self,sentence):\n",
    "        if(os.path.isfile(\"Stopwords/Stopword.txt\")):\n",
    "            STOPWORDS = set(line.strip() for line in open('Stopwords/Stopwords.txt'))\n",
    "        else:\n",
    "            STOPWORDS = set(stopwords.words('german'))\n",
    "        sentence_clean =\"\"\n",
    "        sentence =  [w.lower() for w in sentence.split() if not w.lower() in STOPWORDS] \n",
    "        for word in sentence:\n",
    "            sentence_clean+=word+\" \"\n",
    "        return sentence_clean\n",
    "\n",
    "    def create_sentence_index_list(self, predictions, sentences):\n",
    "        sentenceIndexList = [] \n",
    "        count = 0\n",
    "        for i in range(0,len(predictions)):\n",
    "            if(len(sentences[i].split()) >= self.min_length):\n",
    "                if(predictions[i] == 1):\n",
    "                    sentenceIndexList.append(i)\n",
    "        return sentenceIndexList    \n",
    "    \n",
    "    def read_nlp_processed_game_to_game_patterns(self, processedManual):\n",
    "        gamePattern = game_patterns()\n",
    "        gamePattern.set_ID(processedManual.get_ID())\n",
    "        doc=processedManual.get_spacy_doc()\n",
    "        sentenceTexts = []\n",
    "        for sent in doc.sents:\n",
    "            sentenceTexts.append(str(sent))\n",
    "        gamePattern.set_sentences(sentenceTexts)\n",
    "        \n",
    "        if self.lemmatize:\n",
    "            listLemmaSentences = list(str(sent.lemma_) for sent in doc.sents)\n",
    "        else:\n",
    "            listLemmaSentences = list(str(sent.text) for sent in doc.sents)\n",
    "           \n",
    "        if(len(listLemmaSentences) == 0):\n",
    "            return gamePattern\n",
    "        \n",
    "        if self.remove_stopwords:\n",
    "            listLemmaSentences = [self.clean_stopwords(sent) for sent in listLemmaSentences]\n",
    "                    \n",
    "        for key in self.modelDictionary:\n",
    "            model = self.modelDictionary.get(key)\n",
    "            count_vector = self.counterVectorizeDirectory.get(key)\n",
    "            if model:\n",
    "                testing_data = count_vector.transform(listLemmaSentences)\n",
    "                predictions = model.predict(testing_data)\n",
    "                indexedSentences = self.create_sentence_index_list(predictions,listLemmaSentences)\n",
    "                gamePattern.add_pattern(int(key),indexedSentences)               \n",
    "        return gamePattern\n",
    "                \n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
