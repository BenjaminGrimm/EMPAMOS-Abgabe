{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klasse matcher_localizer\n",
    "\n",
    "Findet Patterns in einer Anleitung mit einem einfachen spacy PhraseMatcher. Zuordnung anhand von Wortlisten (z.B. aus der REST Schnittstelle).\n",
    "\n",
    "\n",
    "Funktionen: \n",
    "\n",
    "    init_matcher(language, pattern_terms)\n",
    "        language:       Sprache als String z.B.: \"de\"\n",
    "        pattern_terms:  Dictionary mit Key: Pattern ID; Value: Liste mit Worten zu dem Pattern. get_all_pattern_evidence_words aus der REST Schnittstelle liefert ein passendes Dictionary\n",
    "    \n",
    "    read_nlp_processed_game_to_game_patterns(nlp_processed_game)\n",
    "    nlp_processed_game:      nlp_processed_game Objekt\n",
    "    returns:                 game_patterns Objekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nicht verwenden!** <br>\n",
    "Der matcher_localizer ist veraltet und sollte nicht länger verwendet werden. Er wurde durch den regex_localizer ersetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider\n",
    "import json\n",
    "#import nltk\n",
    "#from nltk import tokenize\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "%run ./GamePatterns.ipynb\n",
    "%run ./nlpProcessedGame.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class matcher_localizer:\n",
    "    #def __init__ (self, *args, **kwargs):\n",
    "        \n",
    "        \n",
    "    def init_matcher(self,spacy_nlp, pattern_terms):\n",
    "        self.nlp = spacy_nlp\n",
    "        self.pattern_terms = pattern_terms\n",
    "        self.min_length = 0\n",
    "        self.matcher = PhraseMatcher(self.nlp.vocab, attr=\"LOWER\")\n",
    "        #alle Patterns in den Matcher laden\n",
    "        for patternID, terms in pattern_terms.items():\n",
    "            patterns = [self.nlp.make_doc(text) for text in terms]\n",
    "            self.matcher.add(str(patternID),None, *patterns)\n",
    "            \n",
    "    def init_matcher_lemma(self,spacy_nlp, pattern_terms):\n",
    "        self.nlp = spacy_nlp\n",
    "        self.pattern_terms = pattern_terms\n",
    "        self.min_length = 0\n",
    "        self.matcher = PhraseMatcher(self.nlp.vocab, attr=\"LEMMA\")\n",
    "        #alle Patterns in den Matcher laden\n",
    "        for patternID, terms in pattern_terms.items():\n",
    "            patterns = [self.nlp(text) for text in terms]\n",
    "            self.matcher.add(str(patternID),None, *patterns)\n",
    "            \n",
    "    def set_min_sentence_length(self, min_length):\n",
    "        self.min_length = min_length\n",
    "        \n",
    "    def read_nlp_processed_game_to_game_patterns(self,nlp_processed_game):\n",
    "        gp = game_patterns()\n",
    "        gp.set_ID(nlp_processed_game.get_ID())\n",
    "        doc=nlp_processed_game.get_spacy_doc()\n",
    "        #Alle Satzanfänge ermitteln\n",
    "        sentenceStarts = []\n",
    "        sentenceTexts = []\n",
    "        for sent in doc.sents:\n",
    "            sentenceStarts.append(sent.start)\n",
    "            sentenceTexts.append(str(sent))\n",
    "        gp.set_sentences(sentenceTexts)\n",
    "        #print(\"Starts:\")\n",
    "        #print(sentenceStarts)\n",
    "        sentences = {}\n",
    "        for key, value in self.pattern_terms.items():\n",
    "            sentences[str(key)] = []\n",
    "        matches = self.matcher(doc)\n",
    "        currSent = 0\n",
    "        currSentLength = 0\n",
    "        #Alle Sätze durchlaufen und Prüfen, ob das aktuelle Match in diesem Satz vorkommt\n",
    "        #Dabei wird ausgenutzt, dass die Matches sortiert zurück gegeben werden.\n",
    "        #So müssen die Sätze nur ein einziges mal durchlaufen werden.\n",
    "        for match_id, start, end in matches:\n",
    "            #print(\"ID: \"+ nlp.vocab.strings[match_id] + \" start: \" + str(start) + \" end: \"+ str(end)+ \" pattern found: \" + doc[start:end].text)\n",
    "            \n",
    "            #Eins weiter bis der Satz mit dem Match gefunden wurde, oder das Ende erreicht ist.\n",
    "            while currSent <len(sentenceStarts)-1 and sentenceStarts[currSent + 1] <= start:\n",
    "                currSent += 1\n",
    "                if(currSent < len(sentenceStarts)-1):\n",
    "                    currSentLength = sentenceStarts[currSent + 1] - sentenceStarts[currSent]\n",
    "                else:\n",
    "                    currSentLength = len(doc) - sentenceStarts[currSent]\n",
    "            #print(currSentLength)\n",
    "            #Den Satz in die Liste der gefundenen Sätze des Patterns eintragen, aber nur jeweils einmal\n",
    "            if(currSent not in sentences[self.nlp.vocab.strings[match_id]]):\n",
    "                #Mindest Satzlänge beachten\n",
    "                if(currSentLength >= self.min_length):\n",
    "                    sentences[self.nlp.vocab.strings[match_id]].append(currSent)\n",
    "        #print(sentences)\n",
    "\n",
    "        for key, value in sentences.items():\n",
    "            if(len(value)>0):\n",
    "                gp.add_pattern(int(key),value.copy())\n",
    "        return gp\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class matcher_localizer:\n",
    "    #def __init__ (self, *args, **kwargs):\n",
    "        \n",
    "        \n",
    "    def init_matcher(self,spacy_nlp, pattern_terms):\n",
    "        self.nlp = spacy_nlp\n",
    "        self.pattern_terms = pattern_terms\n",
    "        self.matcher = PhraseMatcher(self.nlp.vocab, attr=\"LOWER\")\n",
    "        #alle Patterns in den Matcher laden\n",
    "        for patternID, terms in pattern_terms.items():\n",
    "            patterns = [self.nlp.make_doc(text) for text in terms]\n",
    "            self.matcher.add(str(patternID),None, *patterns)\n",
    "        \n",
    "    def read_nlp_processed_game_to_game_patterns(self,nlp_processed_game):\n",
    "        gp = game_patterns()\n",
    "        gp.set_ID(nlp_processed_game.get_ID())\n",
    "        doc=nlp_processed_game.get_spacy_doc()\n",
    "        #Alle Satzanfänge ermitteln\n",
    "        sentenceStarts = []\n",
    "        sentenceTexts = []\n",
    "        for sent in doc.sents:\n",
    "            sentenceStarts.append(sent.start)\n",
    "            sentenceTexts.append(sent)\n",
    "        gp.set_sentences\n",
    "        sentences = {}\n",
    "        for key, value in self.pattern_terms.items():\n",
    "            sentences[str(key)] = []\n",
    "        matches = self.matcher(doc)\n",
    "        currSent = 0\n",
    "        #Alle Sätze durchlaufen und Prüfen, ob das aktuelle Match in diesem Satz vorkommt\n",
    "        #Dabei wird ausgenutzt, dass die Matches sortiert zurück gegeben werden.\n",
    "        #So müssen die Sätze nur ein einziges mal durchlaufen werden.\n",
    "        for match_id, start, end in matches:\n",
    "            #print(\"ID: \"+ nlp.vocab.strings[match_id] + \" start: \" + str(start) + \" pattern found: \" + doc[start:end].text)\n",
    "            \n",
    "            #Eins weiter bis der Satz mit dem Match gefunden wurde, oder das Ende erreicht ist.\n",
    "            while currSent <len(sentenceStarts)-1 and sentenceStarts[currSent + 1] < start:\n",
    "                currSent += 1\n",
    "            #Den Satz in die Liste der gefundenen Sätze des Patterns eintragen, aber nur jeweils einmal\n",
    "            if(currSent not in sentences[self.nlp.vocab.strings[match_id]]):\n",
    "                sentences[self.nlp.vocab.strings[match_id]].append(currSent)\n",
    "        #print(sentences)\n",
    "\n",
    "        gp.set_sentence_count(len(sentenceStarts))\n",
    "        for key, value in sentences.items():\n",
    "            if(len(value)>0):\n",
    "                gp.add_pattern(int(key),value.copy())\n",
    "        return gp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
