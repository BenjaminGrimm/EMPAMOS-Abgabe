{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook geschieht die eigentliche Arbeit. Verschiedene Localizer können hinzugefügt werden. Durch den Voter werden die einzelnen Localizer gewichtet. Die Resultate werden als game_patterns Objekte gespeichert (Konvertiert zu JSON). Zusätzlich werden Gephi Graphen gesichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.auth import PlainTextAuthProvider\n",
    "from cassandra.cluster import Cluster\n",
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "\n",
    "%run ./REST.ipynb\n",
    "%run ./GamePatterns.ipynb\n",
    "%run ./nlpProcessedGame.ipynb\n",
    "\n",
    "%run ./DistanceCalculator.ipynb\n",
    "%run ./MatcherLocalizer.ipynb\n",
    "%run ./RegexLocalizer.ipynb\n",
    "%run ./GamePatternsVoter.ipynb\n",
    "%run ./BayesLocalizer.ipynb\n",
    "%run ./SVMLocalizer.ipynb\n",
    "%matplotlib inline\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Abschnitt werden Parameter für das gesamte Notebook eingestellt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datenbank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_username = 'itp_mining'\n",
    "db_password = 'mining-data'\n",
    "db_keySpace = 'itp_mining'\n",
    "cluster_address = 'big1.informatik.fh-nuernberg.de'\n",
    "\n",
    "\n",
    "query = \"select * from working_text limit 5\"\n",
    "#query = \"select * from working_text where  sourceuid = '6b05437860f54f19f6eca4b38825807c'\" #Leere Anleitung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_graph = True\n",
    "export_graph = True\n",
    "export_sentences = True\n",
    "\n",
    "#Ausgabeverzeichnisse\n",
    "object_output_directory = \"Output/GamePattern\"\n",
    "object_filename_prefix = \"GamePatterns-\"\n",
    "\n",
    "graph_output_directory = \"Output/GamePatternGraphs\"\n",
    "graph_filename_prefix = \"Graph-\"\n",
    "\n",
    "sentences_output_directory = \"Output/Sentences\"\n",
    "sentences_filename_prefix = \"Sentences-\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Localizer Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length = 3  #Mindest Satzlänge in Tokens. Kürzere Sätze werden beim Lokalisieren ignoriert.\n",
    "\n",
    "#use_matcher_localizer = False\n",
    "#pattern_terms_file_name = \"pattern_terms.json\"\n",
    "#weight_matcher = 1\n",
    "\n",
    "use_regex_localizer = True\n",
    "pattern_expressions_file_name = \"pattern_expressions.json\"\n",
    "weight_regex = 2\n",
    "\n",
    "use_svm_localizer = True\n",
    "svm_models_directory = \"SVM_Models\"\n",
    "weight_svm = 1\n",
    "svm_remove_stopwords = True\n",
    "\n",
    "use_bayes_localizer = True\n",
    "bayes_models_directory = \"Bayes\"\n",
    "weight_bayes = 1\n",
    "bayes_remove_stopwords = True\n",
    "bayes_lemmatize = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voter\n",
    "\n",
    "    use_uniform:\n",
    "        True: Demokratie der einzelnen Localizer mit einfacher Mehrheit. Übrige parameter werden ignoriert\n",
    "        False: Verwendung der einzelnen Gewichte\n",
    "    \n",
    "    use_weight_Ratio: \n",
    "        True: Threshold proportional zur Summe der Gewichte. weight_threshold wird ignoriert\n",
    "        False: Fester Wert für Threshold. weight_Ratio wird ignoriert\n",
    "    \n",
    "    weight_Ratio: \n",
    "        Nur bei use_weight_Ratio = True. Threshold als Anteil von Summe der Gewichte\n",
    "    \n",
    "    weight_threshold:\n",
    "        Nur bei use_weight_Ratio = False. Fester Wert für Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_uniform = False\n",
    "use_weight_Ratio = False\n",
    "weight_Ratio = 0.5\n",
    "weight_threshold = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstandsberechnung<br>\n",
    "Kommt nur bei *export_graph = True* zum Einsatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_threshold = 0.3\n",
    "#distance_fkt = distance_minimum\n",
    "#distance_fkt = distance_average\n",
    "#distance_fkt = distance_hausdorff\n",
    "distance_fkt = distance_cuddle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphische Darstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maximum_edge_size = 3\n",
    "node_size=1000\n",
    "node_color='blue'\n",
    "edge_color='#303060A0'\n",
    "font_size =20\n",
    "alpha=0.5\n",
    "title_size = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REST Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = empamos_rest_client()\n",
    "pattern_names = client.get_patterns_name_dictionary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausgabeverzeichnis erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(graph_output_directory, exist_ok=True)\n",
    "os.makedirs(object_output_directory, exist_ok=True)\n",
    "os.makedirs(sentences_output_directory, exist_ok=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer Initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Arten von Pattern-Localisierern initialisieren<br><br>\n",
    "\n",
    "An dieser Stelle können beliebige Lokalizer hinzugefügt werden.<br>\n",
    "Es wird empfohlen einen Schalterparameter und einen Gewichtungsparameter einzuführen. Diese und weitere Parameter können im Abschnit Parameter unter Localizer Parameter verwaltet werden.<br>\n",
    "Nach der Initialisierung müssen die Localizer in die localizer Liste eingetragen werden. Die Bezeichnung in die localizer_names Liste und das Gewicht in die weights Liste.<br><br>\n",
    "Z.B.:<br>\n",
    "```\n",
    "localizers.append(myLocalizer)\n",
    "localizers_names.append(\"My Localizer Name\")\n",
    "weights.append(weight_myLocalizer)\n",
    "```\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localizers = []\n",
    "localizers_names = []  \n",
    "weights = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~PatternMatcher-Lokalisierer~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deprecated\n",
    "if use_matcher_localizer:\n",
    "    patloc = matcher_localizer()\n",
    "    myFile = open(pattern_terms_file_name, \"r\")\n",
    "    pattern_terms = myFile.read()\n",
    "    pattern_terms = json.loads(pattern_terms)\n",
    "    patloc.init_matcher(nlp,pattern_terms)\n",
    "    localizers.append(patloc)\n",
    "    localizers_names.append(\"Matcher Lower\")\n",
    "    weights.append(weight_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~PatternMatcher-Lokalisierer basierend auf Lemmata~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Deprecated\n",
    "if use_matcher_localizer:\n",
    "    lemloc = matcher_localizer()\n",
    "    myFile = open(pattern_terms_file_name, \"r\")\n",
    "    pattern_terms = myFile.read()\n",
    "    pattern_terms = json.loads(pattern_terms)\n",
    "    lemloc.init_matcher_lemma(nlp,pattern_terms)\n",
    "    localizers.append(lemloc)\n",
    "    localizers_names.append(\"Matcher Lemma\")\n",
    "    weights.append(weight_matcher)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regex Lokalisierer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_regex_localizer:\n",
    "    regloc = regex_localizer()\n",
    "    with open(pattern_expressions_file_name, \"r\") as myFile:\n",
    "        pattern_expressions = myFile.read()\n",
    "    pattern_expressions = json.loads(pattern_expressions)\n",
    "    regloc.init(pattern_expressions,True)\n",
    "    regloc.set_min_sentence_length(min_length)\n",
    "\n",
    "    localizers.append(regloc)\n",
    "    localizers_names.append(\"Regex\")\n",
    "    weights.append(weight_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayes Lokalisierer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if use_bayes_localizer:\n",
    "    bayloc = BayesLocalizer(bayes_remove_stopwords,bayes_lemmatize)\n",
    "    bayloc.load_all_models(bayes_models_directory)\n",
    "\n",
    "\n",
    "    localizers.append(bayloc)\n",
    "    localizers_names.append(\"Bayes\")\n",
    "    weights.append(weight_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes-Lokalisierer Handselected initailisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bayloc = BayesLocalizer() bayloc.load_all_models()\n",
    "localizers.append(bayloc) localizers_names.append(\"Bayes\") weights.append(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Lokalisierer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_svm_localizer:\n",
    "    svmloc = SVMLocalizer()\n",
    "    svmloc.load_all_models(svm_models_directory)\n",
    "    svmloc.remove_stopwords = svm_remove_stopwords\n",
    "    \n",
    "    localizers.append(svmloc)\n",
    "    localizers_names.append(\"SVM\")\n",
    "    weights.append(weight_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Localizer hier einfügen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gegebenenfalls Zielgewicht berechnen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_weight_Ratio == True:\n",
    "    weight_threshold = sum(weights) * weight_Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verbindung mit Datenbank herstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#cluster = Cluster(['172.17.0.3'])\n",
    "#cluster = Cluster(['big1.informatik.fh-nuernberg.de'])\n",
    "#session = cluster.connect('empamos')\n",
    "#ap = PlainTextAuthProvider(username='itp_mining', password='mining-data')\n",
    "#cluster = Cluster(['big1.informatik.fh-nuernberg.de'], auth_provider=ap)\n",
    "#session = cluster.connect('itp_mining')\n",
    "\n",
    "ap = PlainTextAuthProvider(username=db_username, password=db_password)\n",
    "cluster = Cluster([cluster_address], auth_provider=ap)\n",
    "session = cluster.connect(db_keySpace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anleitungen verarbeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gewünschte Anleitungen aus der Datenbank laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = session.execute(query)\n",
    "\n",
    "\n",
    "#rows = session.execute(\"select * from working_text limit 4\")\n",
    "\n",
    "#rows = session.execute(\"select * from working_text where  sourceuid = '61352503f164d56a7d013f29cbe330fb'\") #Hanabi\n",
    "#rows = session.execute(\"select * from working_text where  sourceuid = 'ece99495e009ec6037ef86220d880436'\") #Tzolkin\n",
    "#rows = session.execute(\"select * from working_text where  sourceuid = '46a0e7d814e78b2978f6c703fefefabd'\") #7Wonders\n",
    "#rows = session.execute(\"select * from working_text where  sourceuid = '6b05437860f54f19f6eca4b38825807c'\") #Bug!!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rows[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Debug*<br>\n",
    "Dummy Klasse um Cassandra Resultsetz zu simulieren. Wird verwenden um eigene Texte für Testzwecken zu analysieren.<br>\n",
    "Für den Produktivbetrieb auskommentieren!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class dummy_class:\n",
    "    def dummy(self):\n",
    "        print(\"Dummy\")\n",
    "rows = []\n",
    "rows.append(dummy_class())\n",
    "rows[0].sourceuid = '12345'\n",
    "#rows[0].text =  \"Siegpunkt! Punkt! Siegpunkt! Dies ist ein toller Test. Siegpunkte! Ganz toller Test. Punkte werden gepunktet. Keine Siegpunkte. Nimm den Wertungsblock. Das ein wertungsblock. Das Wetter ist Schön. EMPAMOS RULZ HARDCORE Bestes IT Projekt evar!\"\n",
    "rows[0].text =  \"Jeder Spieler stellt seinen Meeple auf sein Startfeld. Der rote Spieler erhält 5 Eisen. Der blaue Spieler erhält 2 Holz und 2 Getreide. Jeder nimmt sich zwei Aktionskarten. Alle Spieler stellen ihre Superkühe auf die jeweiligen Anfangsfelder. OCRMÜLL. OCRSUPERMÜLL. Der Blaue Spieler beginnt.\"\n",
    "\n",
    "rows[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Anleitungen verarbeiten\n",
    "\n",
    "Je Anleitung:<br>\n",
    "-Anleitung tokenisieren<br>\n",
    "-Tokenisierte Anleitung mit allen Pattern-Lokalisierern durchsuchen<br>\n",
    "-Dabei entstandene GamePattern Objekte mittels Voter in ein einzelnes GamePattern Objekt überführen<br>\n",
    "-Abstände zwischen den einzelnen Patterns berechnen<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_games_patterns = []\n",
    "start = time.time()\n",
    "game_count = 0\n",
    "for row in rows:\n",
    "    #if(len(row.text)==0):\n",
    "        #continue\n",
    "        \n",
    "    print(\"Processing game manual \" + str(row.sourceuid))\n",
    "    game_count += 1\n",
    "    \n",
    "    processed_game = nlp_processed_game()\n",
    "    processed_game.set_ID(row.sourceuid)\n",
    "    processed_game.set_spacy_doc(nlp(row.text))\n",
    "    #print(row.text)\n",
    "    current_game_patterns = []\n",
    "    cnt = 0\n",
    "    for localizer in localizers:\n",
    "        \n",
    "        gp = localizer.read_nlp_processed_game_to_game_patterns(processed_game )   \n",
    "        current_game_patterns.append(gp)\n",
    "        current_name = \"undefined\"\n",
    "        if cnt < len(localizers_names):\n",
    "            current_name = localizers_names[cnt]\n",
    "        cnt += 1\n",
    "        if verbose:\n",
    "            print(current_name)\n",
    "            gp.print_verbose(pattern_names)\n",
    "        \n",
    "    if use_uniform == True:\n",
    "        merged_game_patterns = merge_game_patterns_uniform(current_game_patterns)\n",
    "    else:\n",
    "        merged_game_patterns = merge_game_patterns(current_game_patterns, weights, weight_threshold)\n",
    "    \n",
    "    if verbose:\n",
    "        print()\n",
    "        print(\"Voter result\")\n",
    "        merged_game_patterns.print_verbose(pattern_names)\n",
    "    \n",
    "        print(\"Minimum\")\n",
    "        print(calculate_distance(merged_game_patterns.patterns,distance_minimum))\n",
    "        print(\"\")\n",
    "        print(\"Average\")\n",
    "        print(calculate_distance(merged_game_patterns.patterns,distance_average))\n",
    "        print(\"\")\n",
    "        print(\"Hausdorff\")\n",
    "        print(calculate_distance(merged_game_patterns.patterns,distance_hausdorff))\n",
    "        print(\"\")\n",
    "        print(\"Cuddle\")\n",
    "        print(calculate_distance(merged_game_patterns.patterns,distance_cuddle))\n",
    "        print(\"\")\n",
    "        print(\"\")\n",
    "        \n",
    "    if len(merged_game_patterns.patterns) == 0:\n",
    "        print(\"no pattern found in this game\")\n",
    "        #continue\n",
    "    \n",
    "    all_games_patterns.append(merged_game_patterns)\n",
    "    with open(object_output_directory+\"/\"+ object_filename_prefix+ str(row.sourceuid)+\".json\",\"w\") as f:\n",
    "        f.write(merged_game_patterns.toJSON())\n",
    "        \n",
    "    if export_sentences:\n",
    "        with open(sentences_output_directory+\"/\"+ sentences_filename_prefix + str(row.sourceuid)+\".txt\",\"w\") as f:\n",
    "            f.write(merged_game_patterns.get_sentences_with_patterns(pattern_names))\n",
    "    \n",
    "        \n",
    "    dists = calculate_distance(merged_game_patterns.patterns,distance_fkt)\n",
    "    print(dists)\n",
    "    \n",
    "    if not create_graph:\n",
    "        continue\n",
    "        \n",
    "    # Build your graph\n",
    "    df = pd.DataFrame(columns=['source', 'target', 'value'])\n",
    "    for i in range(0,len(merged_game_patterns.patterns)):\n",
    "        for j in range (i,len(merged_game_patterns.patterns)):\n",
    "            #print(str(list(merged_game_patterns.patterns.keys())[i]) + \"-\" + str(list(merged_game_patterns.patterns.keys())[j]))\n",
    "            d = dists[list(merged_game_patterns.patterns.keys())[i]][list(merged_game_patterns.patterns.keys())[j]] \n",
    "            if d <= distance_threshold or i ==j:    \n",
    "                value = maximum_edge_size * (distance_threshold +0.01 - d) / (distance_threshold +0.01)\n",
    "                s = list(merged_game_patterns.patterns.keys())[i]\n",
    "                t = list(merged_game_patterns.patterns.keys())[j]\n",
    "                new_row = {'source':str(s)+\":\"+pattern_names[s], 'target':str(t)+\":\"+pattern_names[t], 'value':value}\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "    # Build your graph\n",
    "    G = nx.from_pandas_edgelist(df, \n",
    "                                source='source',\n",
    "                                target='target',\n",
    "                                edge_attr=True)\n",
    "    \n",
    "    #Write the Graph in a file\n",
    "    if export_graph:\n",
    "        nx.write_graphml(G,graph_output_directory+\"/\"+ graph_filename_prefix + str(row.sourceuid) + \".graphml\")\n",
    "        \n",
    "    #pos = nx.kamada_kawai_layout(G, weight='value')\n",
    "    pos = nx.circular_layout(G)\n",
    "    _ = plt.figure(figsize=(20, 20))\n",
    "    nx.draw(G, pos, \n",
    "            node_size=node_size, \n",
    "            node_color=node_color,\n",
    "            font_size =font_size,\n",
    "            alpha=alpha,\n",
    "            with_labels = True)\n",
    "    plt.title('Graph Visualization for ' + str(merged_game_patterns.get_ID()), size=title_size)\n",
    "\n",
    "    for (node1,node2,data) in G.edges(data=True):\n",
    "        width = data['value'] \n",
    "        _ = nx.draw_networkx_edges(G,pos,\n",
    "                                   edgelist=[(node1, node2)],\n",
    "                                   width=width,\n",
    "                                   edge_color=edge_color,\n",
    "                                   alpha=0.5)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "end = time.time()\n",
    "print(\"Processed \" + str(game_count) + \" games.\")\n",
    "print(\"Elapsed Time: \" + str(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(all_games_patterns[0].toJSON())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
